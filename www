krsod_i = dfn['krsod']

new_series = krsod_i

count_doc = 0
for krsod in krsod_i:
    krsod = krsod.split()
    for word in krsod:
        count_doc += 1
        if word in dict1:
            dict1[word] += 1
        else: 
            dict1[word] = 1 
            
count_doc

dict2 = {}

krsod = krsod_i[0].split()
count_krsod = len(krsod)
for word in krsod:
    if word in dict2:
        dict2[word] += 1
    else: 
        dict2[word] = 1 
   
   
   
dict2

for key in dict2:
    dict2[key] = (dict2[key] / count_krsod) / (dict1[key] / count_doc)
    
    
for key in dict2.keys():
    if dict2[key] < 10**(-5):
        del dict2[key]
        
'#'.join(dict2.keys())


def key_words(krsod, dict1, count_doc):
    dict2 = {} 
    krsod = krsod.split()
    count_krsod = len(krsod)
    for word in krsod:
        if word in dict2:
            dict2[word] += 1
        else: 
            dict2[word] = 1
    for key in dict2:
        dict2[key] = (dict2[key] / count_krsod) / (dict1[key] / count_doc)
    print(dict2)
    dict3 = {}
    
    for key in dict2.keys():
        if dict2[key] > 100:
            dict3[key] = dict2[key]
    return '#'.join(dict3.keys())
    
    for i in range(len(new_series)):
    new_series[i] = key_words(dfn["krsod"][i], dict1, count_doc)
    
    
    
    
    
    
    dfn['org']=dfn['avt'].str.split('(').str.get(1)
dfn['avt']=dfn['avt'].str.split('(').str.get(0)
dfn['org']=dfn['org'].str.replace(")","")


for raw_text in text_df_description_values:
    raw_text_lower  = raw_text.lower()
    text_by_words = reg_expr_compiled.findall(raw_text_lower)
    corpus.append(text_by_words)
print(corpus)


morph=pymorhy2.MorphAnalyzer()
for token_list in corpus:
    normalized_token_list = []
    for word in token_list:
        parsed_token = morph.parse(word)
        normal_form = parsed_token[0].normal_form
        normalized_token_list.append(normal_form)
    normalized_corpus.append(normalized_token_list)



doc count = len(normalized corpus) 
doc ids = []
tokens = [] 
for doc id in range(doc_count): 
    for token in normalized corpus[doc_id]: 
         doc ids. append(doc id) 
         tokens.append (token) 
tokens df = pd.Dataframe ({ 'doc id': doc ids, 'word': tokens })
tokens df = tokens df.assign(dummy = 1) 
tokens df. head)|
